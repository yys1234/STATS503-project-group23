---
title: "stats531 final project appendix"
author: "Li-Hsuan Lin, Yushan Yang , Chalida Naiyaporn"
date: "4/21/2022"
output: html_document
---



```{r}
# load the library
library(dplyr)
library(GGally)
library(randomForest)
library(gbm)
library(e1071)
library("vcd")
```


```{r}
# read in the data
ecommerce = read.csv('./data/Train.csv')
```


```{r}
# turn the categorical columns into factor variables
ecommerce$Reached.on.Time_Y.N <- factor(ecommerce$Reached.on.Time_Y.N)
levels(ecommerce$Reached.on.Time_Y.N) <- c("Yes", "No")
ecommerce$Warehouse_block <- factor(ecommerce$Warehouse_block)
ecommerce$Mode_of_Shipment <- factor(ecommerce$Mode_of_Shipment)
ecommerce$Gender <- factor(ecommerce$Gender)
```


```{r}
# as the predictor "Product_importance" is an ordinal variable, we turn it into numeric value 1, 2, 3
ecommerce$Product_importance <- factor(ecommerce$Product_importance)

ecommerce = ecommerce %>% select("Warehouse_block", "Mode_of_Shipment", "Customer_care_calls", "Customer_rating",
                                 "Cost_of_the_Product", "Prior_purchases", "Product_importance", "Gender",
                                 "Discount_offered", "Weight_in_gms", "Reached.on.Time_Y.N")
```

```{r}
# divide data to train and test
set.seed(503)
data <- ecommerce %>% mutate(id=row_number())
train <- data %>% group_by(Reached.on.Time_Y.N) %>% sample_frac(0.7) %>% ungroup()
test <- anti_join(data, train, by = 'id')
train <- dplyr::select(train, -id)
test <- dplyr::select(test, -id)
```


```{r}
# get the summary of data
summary(data)
summary(train)

cols = c("Customer_care_calls","Customer_rating", "Cost_of_the_Product", "Prior_purchases",
        "Discount_offered", "Weight_in_gms")

cor(train[,cols])

```



```{r}
# correlation
X <- train[,cols]
library(corrplot)
corr<-cor(X)
corrplot(corr,method='circle')
ggpairs(train, columns = cols, ggplot2::aes(colour=Reached.on.Time_Y.N))
```


```{r}
# exploratory plots
plt1 <- ggplot(data = ecommerce) + geom_bar(mapping = aes(x = Warehouse_block, fill = Reached.on.Time_Y.N), position = 'dodge') + 
    labs(x = 'Warehouse block') + theme_bw()+ theme(axis.title.y=element_blank())
plt2 <- ggplot(data = ecommerce) + geom_bar(mapping = aes(x = Mode_of_Shipment, fill = Reached.on.Time_Y.N), position = 'dodge') + 
    labs(x = 'Mode of shipment')  + theme_bw()+ theme(axis.title.y=element_blank())
plt3 <- ggplot(data = ecommerce) + geom_boxplot(mapping = aes(x = Customer_care_calls, y = Reached.on.Time_Y.N)) +
    labs(x = 'Customer care calls')  + theme_bw() + theme(axis.title.y=element_blank())
plt4 <- ggplot(data = ecommerce) + geom_boxplot(mapping = aes(x = Customer_rating, y = Reached.on.Time_Y.N)) +
    labs(x = 'Customer rating') + theme_bw()+ theme(axis.title.y=element_blank())
plt5 <- ggplot(data = ecommerce) + geom_boxplot(mapping = aes(x = Cost_of_the_Product, y = Reached.on.Time_Y.N)) +
    labs(x = 'Cost of the product')  + theme_bw()+ theme(axis.title.y=element_blank())
plt6 <- ggplot(data = ecommerce) + geom_boxplot(mapping = aes(x = Prior_purchases, y = Reached.on.Time_Y.N)) +
    labs(x = 'Prior purchases')  + theme_bw()+ theme(axis.title.y=element_blank())
plt7 <- ggplot(data = ecommerce) + geom_bar(mapping = aes(x = Product_importance, fill = Reached.on.Time_Y.N), position = 'dodge') + 
    labs(x = 'Product importance')  + theme_bw()+ theme(axis.title.y=element_blank())
plt8 <- ggplot(data = ecommerce) + geom_bar(mapping = aes(x = Gender, fill = Reached.on.Time_Y.N), position = 'dodge') + 
    labs(x = 'Gender') + theme_bw()+ theme(axis.title.y=element_blank())
plt9 <- ggplot(data = ecommerce) + geom_boxplot(mapping = aes(x = Discount_offered, y = Reached.on.Time_Y.N)) +
    labs(x = 'Discount offered') +theme_bw()+ theme(axis.title.y=element_blank())
plt10 <- ggplot(data = ecommerce) + geom_boxplot(mapping = aes(x = Weight_in_gms, y = Reached.on.Time_Y.N)) +
    labs(x = 'Weight in gms',)  + theme_bw()+ theme(axis.title.y=element_blank())
```



```{r}
# Export plot
png(file="01_boxplot_hist.png",width=500,height=500,res=100)
ggarrange(plt5,plt9,plt10,  nrow=3, ncol=1, common.legend=T, legend="bottom")
dev.off()

png(file="01_boxplot_hist.png",width=500,height=500,res=100)
ggarrange(plt5, plt1, plt9, plt2, plt10, plt7,  nrow=3, ncol=2, common.legend=T, legend="bottom")
dev.off()

plt11 = ggplot(data = ecommerce) + 
geom_point(aes(x = Weight_in_gms, y = Discount_offered, colour = Reached.on.Time_Y.N))+
xlab('Weight in gms') + ylab('Discount offered')+
theme_bw()

png(file="03_scatter_plot.png",width=500,height=500,res=100)
ggarrange(plt11, common.legend=T, legend="bottom")
dev.off()
```



## Random Forest


There are three parameters in RF for us to adjust: mtry, nodesize and ntree. We use 5-fold cross validation to pick the best parameter.


```{r}
# 5-fold cross validation and calculate error 
# code is used from stats 503 lab
CV_RF <- function(train, mtry, nodesize, ntree){
    
    fold_size = floor(nrow(train)/5)
    cv_error = rep(0,5)
    
    for(i in 1:5){
        if(i!=5){
          CV_test_id = ((i-1)*fold_size+1):(i*fold_size)
        }else{
          CV_test_id = ((i-1)*fold_size+1):nrow(train)
        }
        
        CV_train = train[-CV_test_id,]
        CV_test = train[CV_test_id,]
        
        set.seed(503)
        rf_reach = randomForest(Reached.on.Time_Y.N ~ ., data = CV_train, mtry = mtry, nodesize = nodesize, ntree = ntree, 
                               importance = FALSE)
        rf_test_pred = predict(rf_reach, newdata = CV_test)
        cv_error[i] = mean(rf_test_pred != CV_test$Reached.on.Time_Y.N)
    }
    return(mean(cv_error))
}
```


``` {r}
# use the function above to pick the best parameters (mtry: 3; nodesize: 0.5,1,2; ntr: 500,1000,2000)
# code is used from stats 503 lab

error = array(rep(0,1*3*3), c(1, 3, 3))
for(i in 1:1){
    for(j in 1:3){
        for(k in 1:3){
            error[i,j,k] = CV_RF(train, 3, 2**(j-1)*0.5, 2**(k-1)*500)
        }
    }
}
````


```{r}
# find out the best parameter by looking at the smallest error
which(error == min(error), arr.ind = TRUE) 
```


```{r}
# continue finding the best parameter (mtry: 3; nodesize: 1,2; ntree: 2000,4000)
# use stats 503 lab code

error = array(rep(0,1*2*2), c(1, 2, 2))
for(i in 1:1){
    for(j in 1:2){
        for(k in 1:2){
            error[i,j,k] = CV_RF(train, 3, j, 2000*k)
        }
    }
}
```


```{r}
# find out the best parameter by looking at the smallest error
which(error == min(error), arr.ind = TRUE) # finally choose mtry=1, nodesize=1, ntree=2000
```



```{r}
# continue finding the best parameter (mtry: 3; nodesize: 2; ntree: 4000,5000,6000)
# use stats 503 lab code

error = array(rep(0,1*1*3), c(1, 1, 3))
for(i in 1:1){
    for(j in 1:1){
        for(k in 1:3){
            error[i,j,k] = CV_RF(train, 3, 2, 3000+1000*k)
        }
    }
}

# find out the best parameter by looking at the smallest error
which(error == min(error), arr.ind = TRUE) # finally choose mtry=1, nodesize=1, ntree=2000
```

```{r}
print(error)
```

```{r}
# train on the whole training dataset and predict the test data
set.seed(503)
rf_reach = randomForest(Reached.on.Time_Y.N ~ ., data = train, mtry = 3, nodesize = 2, ntree = 5000, 
                               importance = TRUE)
rf_test_pred = predict(rf_reach, newdata = test)
rf_test_err = mean(rf_test_pred != test$Reached.on.Time_Y.N)
print(rf_test_err) # total test error
```


```{r}
# getting the error within classes
table(rf_test_pred, test$Reached.on.Time_Y.N)
print(317/(317+1014)) # Yes class test error
print(764/(764+1205)) # No class test error
```


```{r}
rf_train_pred = predict(rf_reach, newdata = train)
rf_train_err = mean(rf_train_pred != train$Reached.on.Time_Y.N)
print(rf_train_err) # total train error
```


```{r}
table(rf_train_pred, train$Reached.on.Time_Y.N)
print(286/(2819+286)) # Yes class train error
print(886/(3708+886)) # No class train error
```

```{r}
# check the importance of each predicator
importance(rf_reach)
```




```{r}
# plot the variable importance by MeanDecreaseAccuracy
png(file="Variable importance by MeanDecreaseAccuracy.png",width=800,height=600,res=100)
varImpPlot(rf_reach, type=1, main="Variable importance by MeanDecreaseAccuracy")
dev.off()
```



## AdaBoost


```{r}
# transfer the categorical response to 0,1 representation
train$Reached.on.Time_Y.N <- ifelse(train$Reached.on.Time_Y.N=="Yes",0,1)
test$Reached.on.Time_Y.N <- ifelse(test$Reached.on.Time_Y.N=="Yes",0,1)
```


``` {r}
# 5-fold cross validation and calculate error 
# code is used from the lab
# use stats 503 lab code

CV_Ada <- function(train, n_tree, inter_depth, shrink){
    
    fold_size = floor(nrow(train)/5)
    cv_error = rep(0,5)
    
    for(i in 1:5){
        # iteratively select 4 folds as training data in CV procedure, remaining as test data.
        if(i!=5){
          CV_test_id = ((i-1)*fold_size+1):(i*fold_size)
        }else{
          CV_test_id = ((i-1)*fold_size+1):nrow(train)
        }
        
        CV_train = train[-CV_test_id,]
        CV_test = train[CV_test_id,]
        
        set.seed(503)
        ada_reach = gbm(Reached.on.Time_Y.N~., data = CV_train, distribution = "adaboost", n.trees = n_tree,
                        interaction.depth = inter_depth, shrinkage = shrink)
        ada_test_pred_response = predict(ada_reach, newdata = CV_test, n.trees = n_tree, type="response")
        ada_test_pred <- ifelse(ada_test_pred_response>0.5,1,0)
        cv_error[i] = mean(ada_test_pred != CV_test$Reached.on.Time_Y.N)
    }
    return(mean(cv_error))
}
```


``` {r}
# use the function above to pick the best parameters (n.trees: 500,1000,2000; interaction.depth: 2,3; 
# shrinkage: 0.02,0.06,0.1)
# use stats 503 lab code

error = array(rep(0,3*2*3), c(3, 2, 3))
for(i in 1:3){
    for(j in 1:2){
        for(k in 1:3){
            error[i,j,k] = CV_Ada(train, 2**(i-1)*500, j+1, (k-1)*0.04+0.02)
        }
    }
}
```


``` {r}
# find out the best parameter by looking at the smallest error
which(error == min(error), arr.ind = TRUE) 
```


``` {r}
# continue finding the best parameter (n.trees:250,500; interaction.depth: 2; shrinkage: 0.02,0.04,0.06,0.08)
# use stats 503 lab code

error = array(rep(0,2*1*4), c(2, 1, 4))
for(i in 1:2){
    for(j in 1:1){
        for(k in 1:4){
            error[i,j,k] = CV_Ada(train, 250*i, 2, 0.02*k)
        }
    }
}
```

``` {r}
# find out the best parameter by looking at the smallest error
which(error == min(error), arr.ind = TRUE) 
```


``` {r}
# continue finding the best parameter (n.trees:100,250,400; interaction.depth: 2; shrinkage: 0.05,0.06,0.07)
# use stats 503 lab code

error = array(rep(0,3*1*3), c(3, 1, 3))
for(i in 1:3){
    for(j in 1:1){
        for(k in 1:3){
            error[i,j,k] = CV_Ada(train, 100+150*(i-1), 2, 0.05+0.01*(k-1))
        }
    }
}
```


``` {r}
# find out the best parameter by looking at the smallest error
which(error == min(error), arr.ind = TRUE) 
```


``` {r}
# continue finding the best parameter (n.trees:250; interaction.depth: 2; shrinkage: 0.045,0.05,0.055)
# use stats 503 lab code

error = array(rep(0,1*1*3), c(1, 1, 3))
for(i in 1:1){
    for(j in 1:1){
        for(k in 1:3){
            error[i,j,k] = CV_Ada(train, 250, 2, 0.045+0.005*(k-1))
        }
    }
}
```



``` {r}
which(error == min(error), arr.ind = TRUE) # finally choose n.trees=250, interaction.depth=2, shrinkage=0.05
```


``` {r}
set.seed(503)
ada_reach = gbm(Reached.on.Time_Y.N~., data = train, distribution = "adaboost", n.trees = 250,
                interaction.depth = 2, shrinkage = 0.05)
ada_test_pred_response = predict(ada_reach, newdata = test, n.trees = 250, type="response")
ada_test_pred <- ifelse(ada_test_pred_response>0.5,1,0)
ada_test_err = mean(ada_test_pred != test$Reached.on.Time_Y.N)
print(ada_test_err) # overall test error
```


``` {r}
table(ada_test_pred,test$Reached.on.Time_Y.N)
```


``` {r}
print(247/(247+1084)) # yes class test error
print(815/(815+1154)) # no class test error
```



``` {r}
ada_train_pred_response = predict(ada_reach, newdata = train, n.trees = 250, type="response")
ada_train_pred <- ifelse(ada_train_pred_response>0.5,1,0)
ada_train_err = mean(ada_train_pred != train$Reached.on.Time_Y.N)
print(ada_train_err) # overall training error
```


``` {r}
table(ada_train_pred,train$Reached.on.Time_Y.N)
```

``` {r}
print(498/(498+2607)) # yes class training error
print(1850/(1850+2744)) # no class training error
```


``` {r}
summary(ada_reach)
```


## KNN

``` {r}
library(class)
```


``` {r}
# use one hot encoding for categorical variables with more than 2 different values
library(mltools)
library(data.table)
new_train <- one_hot(as.data.table(train[,1:2]))
new_test <- one_hot(as.data.table(test[,1:2]))
```


``` {r}
# normalize and standarize the numeric column
mean_train = colMeans(train[,c(3:7,9:10)])
sd_train = apply(train[,c(3:7,9:10)],2,sd)
train_x <- scale(train[,c(3:7,9:10)],center = mean_train,scale = sd_train)
test_x <- scale(test[,c(3:7,9:10)],center = mean_train,scale = sd_train)
```


``` {r}
# combine other columns with one hot encoding columns
train_x <- cbind(new_train, train_x)
test_x <- cbind(new_test, test_x)
# change the categorical column with only 2 different values to 0,1
train_x$Gender <- ifelse(train$Gender == 'M',0,1)
test_x$Gender <- ifelse(test$Gender == 'M',0,1)
```


``` {r}
# take out labels
train_label = train %>% .$Reached.on.Time_Y.N
test_label = test %>% .$Reached.on.Time_Y.N
```


``` {r}
# kfold CV function
# use stats 503 lab code
CV_knn <- function(train,train_label,k){
    fold_size = floor(nrow(train)/5)
    cv_error = rep(0,5)
    
    for(i in 1:5){
    if(i!=5){
      CV_test_id = ((i-1)*fold_size+1):(i*fold_size)
    }else{
      CV_test_id = ((i-1)*fold_size+1):nrow(train)
    }
        
    CV_train = train[-CV_test_id,]
    CV_test = train[CV_test_id,]
        
    mean_CV_train = colMeans(CV_train)
    sd_CV_train = apply(CV_train,2,sd)
    CV_train = scale(CV_train,center = mean_CV_train,scale = sd_CV_train)
    CV_test = scale(CV_test,center = mean_CV_train,scale = sd_CV_train)
        
    pred_CV_test = knn(CV_train,CV_test,train_label[-CV_test_id],k = k)
    cv_error[i] = mean(pred_CV_test!=train_label[CV_test_id])
  }
  return(mean(cv_error))
}
```


``` {r}
# 5-fold CV to pick best k
# use stats 503 lab code

k <- seq(from=2, to=200, by=3)
cv_error <- vector(length=length(k))
for(i in 1:length(k)){
  cv_error[i] <- CV_knn(train = train_x, train_label = train_label,k=k[i])
}
```



``` {r}
# Plot error under differet K
library(tidyverse)
error2 <- data.frame(k=k, five.fold=cv_error)
error_plt2 <- error2 %>% gather(key="Method", value="Error", five.fold)
png(file="5-fold CV for KNN.png",width=600,height=400,res=100)
ggplot(data=error_plt2) + geom_point(aes(x=k, y=Error, col=Method)) + geom_line(aes(x=k, y=Error, col=Method)) + 
        scale_color_discrete(labels=c("5-fold")) + ggtitle("Errors for 5-fold CV versus k") + theme_bw()
dev.off()
```



``` {r}
k_best <- error2 %>% dplyr::select(five.fold) %>% sapply(function(x)k[which.min(x)])
print(k_best) # the best K is 68
```



``` {r}
pred_train <- knn(train_x, train_x, train_label, k=89)
pred_test <- knn(train_x, test_x, train_label, k=89)
knn_train_error <- mean(pred_train != train_label)
knn_test_error <- mean(pred_test != test_label)
print(knn_train_error) # total train error
print(knn_test_error) # total test error
```


``` {r}
table(pred_train, train_label)
```

``` {r}
print(587/(587+2518)) # Yes class train error
print(1866/(1866+2728)) # No class train error
```

``` {r}
table(pred_test, test_label)
```

```{r}
print(320/(320+1011)) # Yes class test error
print(805/(1164+805)) # No class test error
```


## SVM


```{r}
# pre-process data

train_sub = train %>% select("Warehouse_block","Mode_of_Shipment","Product_importance","Reached.on.Time_Y.N")
train_vec = train %>% select ("Customer_care_calls","Customer_rating","Cost_of_the_Product",
                              "Prior_purchases","Discount_offered","Weight_in_gms")
scaled_train <- as.data.frame(scale(train_vec))
train = cbind(train_sub,scaled_train)

test_sub = test %>% select("Warehouse_block","Mode_of_Shipment","Product_importance","Reached.on.Time_Y.N")
test_vec = test %>% select ("Customer_care_calls","Customer_rating","Cost_of_the_Product",
                              "Prior_purchases","Discount_offered","Weight_in_gms")
scaled_test <- as.data.frame(scale(test_vec))
test = cbind(test_sub,scaled_test)
```


```{r}
# tune SVM model
tc = tune.control(cross = 5)

tune.out = tune(svm,Reached.on.Time_Y.N~.,data=train, ranges=list(cost=c(0.1,1,5,10),gamma=c(0.1,0.5,1,5),degree=c(1,2,3,4),kernel=c("linear","radial","polynomial")), tunecontrol= tc,scale=F)

```


```{r}
# use best model to predict for training data
bestmod=tune.out$best.model
ypred_train=predict(bestmod ,train)
table(predict=ypred_train, truth=train$Reached.on.Time_Y.N)
train_pred = predict(bestmod, newdata = train)
sum((train_pred != train$Reached.on.Time_Y.N)) / nrow(train)
```


```{r}
# prediction for test data
bestmod=tune.out$best.model
ypred_test=predict(bestmod ,test)
table(predict=ypred_test, truth=test$Reached.on.Time_Y.N)
test_pred = predict(bestmod, newdata = test)
sum((test_pred != test$Reached.on.Time_Y.N)) / nrow(test)
```



## logistic regreesion

```{r}
# training model
train_logistic = glm(Reached.on.Time_Y.N~.,data = train,family = binomial)
train_predProbs = binomial()$linkinv(predict(train_logistic,train))
train_logisticPred = rep(0,nrow(train))
train_logisticPred[train_predProbs>0.5] = 1
```

```{r} 
# construct confusion matrix for training data

table(train_logisticPred,train$Reached.on.Time_Y.N,dnn=c("Predicted","Actual"))
```


```{r}
# training error
(1433+1300)/(1805+3161+1433+1300)
```



```{r}
# construct confusion matrix for testing data

test_predProbs = binomial()$linkinv(predict(train_logistic,test))
test_logisticPred = rep(0,nrow(test))
test_logisticPred[test_predProbs>0.5] = 1
```

```{r}
table(test_logisticPred,test$Reached.on.Time_Y.N,dnn=c("Predicted","Actual"))
```

```{r}
# testing error
(647+551)/(647+551+1322+780)
```















